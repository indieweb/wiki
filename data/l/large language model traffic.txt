https://indieweb.org/large_language_model_traffic

{{stub}}

'''<dfn>[[large language model|Large language model]] traffic</dfn>''' is (AKA <dfn><abbr>LLM</abbr> crawling</dfn> or <dfn>AI scraping</dfn>) the phenomenon of web requests made by LLM operators to ingest data for use in training AI models, or to answer user queries in “search” mode (like ChatGPT Search), which tend to ignore [[robotstxt|robots directives]] and overwhelm  personal blogs, community sites, and other smaller websites.

Increasing usage of LLM tools corresponds with increased [[crawler]] traffic from web [[robot|robots]]. [[website]] owners have noticed spikes in traffic which have made some public sites unavailable. Some crawlers appear not to respect [[robots txt|robots.txt]] directives.

== IndieWeb Examples ==

Several members of the IndieWeb community have reported AI traffic becoming an increasing burden on their servers.

* {{artlung}} was told by system administrators from his [[hosting|web host]] that bot spikes accounted for multiple crashes of Apache.
* As a result of heavy traffic on the indieweb.org wiki, web server nginx rules were added to prevent access to "diff" pages for individual wiki pages for not-logged in user agents
* {{capjamesg}} prefers AI bots not to crawl his website. Because of accounts that some AI bots do not adhere to robots.txt (i.e. https://blog.cloudflare.com/perplexity-is-using-stealth-undeclared-crawlers-to-evade-website-no-crawl-directives/), James blocks many AI bot user agents using nginx. Matching user agents are returned a 404 for every web page.
* [[User:Jak2k.eu|Jak2k]] does not want AI bots to crawl his website. He used [https://anubis.techaro.lol/ Anubis] in the past, but switched to [https://iocaine.madhouse-project.org/ Iocaine] because it does not require client-side JavaScript and can also poison AIs.
* {{aaronpk}} added an nginx rule to block access to the indieweb.org history pages for not-logged-in users in order to combat traffic slowing down the wiki
* The [https://openid.net OpenID.net] website has been regularly taken offline throughout September-October 2025 and the admins have been working to add a CDN in front of the site.
* {{rossabaker}} is using [https://iocaine.madhouse-project.org/ Iocaine] to protect his static site and self-hosted services.  In November, 2025, he is averaging over 20 million requests per day, 97.8% of which are deemed garbage.  Because his code forge gets hit hardest, he generates fake responses from [https://github.com/coding-horror/basic-computer-games BASIC Computer Games], which can be sampled on [https://maze.rossabaker.com his public maze]. 

* {{addyourself}}

== News reporting ==
<!-- recommend using https://indieweb.org/Template:citation when adding new articles, including an archive URL -->
* {{citation
| title = We’re Walling Off The Open Internet To Stop AI—And It May End Up Breaking Everything Else
| url = https://www.techdirt.com/2025/09/08/were-walling-off-the-open-internet-to-stop-ai-and-it-may-end-up-breaking-everything-else/
| author = [https://www.techdirt.com/user/mmasnick/ Mike Masnick]
| published = 2025-09-08
| archiveurl = http://web.archive.org/web/20250919005945/https://www.techdirt.com/2025/09/08/were-walling-off-the-open-internet-to-stop-ai-and-it-may-end-up-breaking-everything-else/
}}
* {{citation
| title = The Open-Source Software Saving the Internet From AI Bot Scrapers
| url = https://www.404media.co/the-open-source-software-saving-the-internet-from-ai-bot-scrapers/
| author = [https://www.404media.co/author/emanuel-maiberg/ Emanuel Maiberg]
| published = 2025-07-07
| archiveurl = https://web.archive.org/web/20250827212038/https://www.404media.co/the-open-source-software-saving-the-internet-from-ai-bot-scrapers/
}}
* {{citation
| title = AI bots strain Wikimedia as bandwidth surges 50%
| url = https://arstechnica.com/information-technology/2025/04/ai-bots-strain-wikimedia-as-bandwidth-surges-50/
| author = [https://arstechnica.com/author/benjedwards/ Benj Edwards]
| published = 2025-04-02
| archiveurl = https://web.archive.org/web/20250819214210/https://arstechnica.com/information-technology/2025/04/ai-bots-strain-wikimedia-as-bandwidth-surges-50/
}}
* {{citation
| title = Open source devs say AI crawlers dominate traffic, forcing blocks on entire countries
| url = https://arstechnica.com/ai/2025/03/devs-say-ai-crawlers-dominate-traffic-forcing-blocks-on-entire-countries/
| author = [https://arstechnica.com/author/benjedwards/ Benj Edwards]
| published = 2025-03-25
| archiveurl = http://web.archive.org/web/20250916022653/https://arstechnica.com/ai/2025/03/devs-say-ai-crawlers-dominate-traffic-forcing-blocks-on-entire-countries/
}}
* {{citation
| title = Cloudflare turns AI against itself with endless maze of irrelevant facts
| url = https://arstechnica.com/ai/2025/03/cloudflare-turns-ai-against-itself-with-endless-maze-of-irrelevant-facts/
| author = [https://arstechnica.com/author/benjedwards/ Benj Edwards]
| published = 2025-03-21
| archiveurl = https://web.archive.org/web/20250327162626/https://arstechnica.com/ai/2025/03/cloudflare-turns-ai-against-itself-with-endless-maze-of-irrelevant-facts/
}}

* {{citation
| title = Are AI Bots Knocking Digital Collections Offline? An Interview with Michael Weinberg
| url = https://scholarlykitchen.sspnet.org/2025/06/23/are-ai-bots-knocking-digital-collections-offline/
| author = [https://scholarlykitchen.sspnet.org/author/lisa-janicke-hinchliffe/ Lisa Janicke Hinchliffe]
| published = 2025-06-23
| archiveurl = http://web.archive.org/web/20250717051935/https://scholarlykitchen.sspnet.org/2025/06/23/are-ai-bots-knocking-digital-collections-offline/
}}
* {{citation
| title = AI Has Created a Battle Over Web Crawling: Training data may wind up in short supply as websites restrict crawler bots
| url = https://spectrum.ieee.org/web-crawling
| author = [https://spectrum.ieee.org/u/eliza-strickland Eliza Strickland]
| published = 2024-08-31
| archiveurl = http://web.archive.org/web/20250912070229/https://spectrum.ieee.org/web-crawling
}}

== Traffic reports ==
* 2025-09-10 [https://flowingdata.com/2025/09/10/bots-account-for-almost-a-third-of-web-traffic/ Bots account for almost a third of web traffic] (summary of [[Cloudflare]] data)
* 2025-09-08 ''[https://tollbit.com/bots/25q2/ AI Scraping Is On The Rise]'' ([https://tollbit.com/ TollBit] State of the Bots - Q2 2025)
* {{citation
| title = 
New Fastly Threat Research Reveals AI crawlers make up almost 80% of AI bot traffic, Meta Leads AI Crawling As ChatGPT Dominates Real-Time Web Traffic
| url = https://www.fastly.com/press/press-releases/new-fastly-threat-research-reveals-ai-crawlers-make-up-almost-80-of-ai-bot
| published = 2025-08-19
| archiveurl = https://web.archive.org/web/20250822232826/https://www.fastly.com/press/press-releases/new-fastly-threat-research-reveals-ai-crawlers-make-up-almost-80-of-ai-bot
}}
* 2025-07-21 [https://www.libraryjournal.com/story/ai-bots-swarm-library-cultural-heritage-sites-causing-slowdowns-and-crashes AI Bots Swarm Library, Cultural Heritage Sites, Causing Slowdowns and Crashes]
* [https://library.duke.edu/using/off-campus/impact-of-bots Impact of AI Bots on Library Websites] (Duke University) ''The overwhelming number of simultaneous requests that these AI bots make on our websites has sometimes rendered them inaccessible to our human library patrons.''
* 2025-04-01 [https://diff.wikimedia.org/2025/04/01/how-crawlers-impact-the-operations-of-the-wikimedia-projects/ How crawlers impact the operations of the Wikimedia projects] ''We are observing a significant increase in request volume, with most of this traffic being driven by scraping bots collecting training data for large language models (LLMs) and other use cases.''
* 2025-04-23 [https://www.inmotionhosting.com/blog/ai-crawlers-slowing-down-your-website/ Why AI Crawlers Are Slowing Down Your Website: The Case for Dedicated Hosting Solutions]
* 2025-10-16 [https://www.404media.co/wikipedia-says-ai-is-causing-a-dangerous-decline-in-human-visitors/ Wikipedia Says AI Is Causing a Dangerous Decline in Human Visitors]
* 2025-10-29 [https://herman.bearblog.dev/agressive-bots/ Aggressive bots ruined my weekend]
** ''Previously'' 2025-03-26 [https://herman.bearblog.dev/the-great-scrape/ The Great Scrape]
* 2025-10-25 [https://xoxo.zone/@Codeberg@social.anoxinon.de/115435663091938788  Codeberg traffic degradations and their mitigations]
* 2025-11-18 [[Pillowfort]] has attributed bot traffic as a reason for [https://www.pillowfort.social/posts/6879553 site slowness and downtime]
* 2025-11-12 [https://bruceediger.com/posts/goofing-on-meta/ Goofing on Meta's AI Crawler] includes a graph of requests per day
* 2025-12-12 [https://www.jwz.org/blog/2025/12/under-attack-please-stand-by-2/ Under Attack, Please Stand By] (''has some interesting comments on the post including about [[Apache]] modules and other mitigation tools'')

== Tools to combat ==
{{section-stub}}

* <dfn id="Anubis">[https://github.com/TecharoHQ/anubis Anubis]</dfn> is a "Web AI Firewall Utility". The "[https://anubis.techaro.lol/docs/design/how-anubis-works How Anubis works]" documentation says that Anubis "uses a proof-of-work challenge to ensure that clients are using a modern browser and are able to calculate SHA-256 checksums".
** [https://social.anoxinon.de/@Codeberg/115033796075422170 Codeberg reported that at least one AI crawler was capable of bypassing Anubis],:
<blockquote>…we can confirm that at least Huawei networks now send the challenge responses and they actually do seem to take a few seconds to actually compute the answers… we assume that AI crawlers leveled up their computing power to emulate more of real browser behaviour </blockquote>
** Anubis can be configured to work without [[JavaScript]] using <code>[https://anubis.techaro.lol/docs/admin/configuration/challenges/metarefresh/ challenge.algorithm.metarefresh]</code>
** flexagoon argues that [https://fxgn.dev/blog/anubis/ you don't need Anubis] for scrapers
* [[Cloudflare]] has a feature called [https://developers.cloudflare.com/bots/additional-configurations/block-ai-bots/ Block AI Bots] which you can use to block AI bots.
* [[fail2ban]] bans logins from IP addresses that show signs of malicious activity
* [https://iocaine.madhouse-project.org/ Iocaine] is simiar to Anubis, but uses scriptable bot recognition on the server only and responds with garbage text to recognized bots with the goal of poisoning AIs.
* [https://github.com/ai-robots-txt/ai.robots.txt ai.robots.txt] lists user agents and their sources to help write [[server|web server]] blocking rules such as in [[htaccess]] rules
* Feeding crawling entities [https://ache.one/notes/html_zip_bomb zip bombs] to ''attempt to exhaust their RAM''
* How to block LLM scrapers in [[11ty]]: 2024-04-15 [https://evilgeniuschronicles.org/posts/2024/04/15/blockin-bots-with-eleventy/ Blockin' Bots with Eleventy]
* [https://arstechnica.com/tech-policy/2025/01/ai-haters-build-tarpits-to-trap-and-trick-ai-scrapers-that-ignore-robots-txt/ AI haters build tarpits to trap and trick AI scrapers that ignore robots.txt]
* [https://petermolnar.net/article/anti-ai-nepenthes-fail2ban/index.html Example setup of an anti-AI tarpit on a self-hosted indieweb site]
* [https://www.owl.is/blogg/blocking-crawlers-without-javascript/ Blocking LLM crawlers, without JavaScript]

== Proposed Tools ==
* [https://blog.cloudflare.com/introducing-pay-per-crawl/ Introducing pay per crawl: Enabling content owners to charge AI crawlers for access]
* [https://rslstandard.org RSL: Really Simple Licensing] ''RSL is an open standard that lets publishers define machine-readable licensing terms for their content, including attribution, pay per crawl, and pay per inference compensation.''
** ''[https://blog.cloudflare.com/content-signals-policy/#how-to-add-content-signals-to-your-website How to add content signals to your website]''
* [https://datatracker.ietf.org/doc/charter-ietf-aipref/ AI Preferences] [[IETF]] working group
* There is a magic string to prevent [https://claude.com/product/claude-code Claude Code] from accessing a site or open source project [Blocking Claude https://aphyr.com/posts/403-blocking-claude] ~ [https://platform.claude.com/docs/en/test-and-evaluate/strengthen-guardrails/handle-streaming-refusals Source]

== Robots.txt Guidance to Scrapers ==
Since 1994, [[robots txt|Robots.txt]] has been the [[sidefile]] mechanism site operators may use to ''suggest'' to web ''spiders'' how to crawl their sites.

* [[user-agent|User agent]] values can be called out by specific name and told whether they are allowed or disallowed.

* [https://contentsignals.org Content-Signals], from [[Cloudflare]] proposes a [[robots txt|robots.txt]] a specific directive called <code>Content-Signal</code> with values of <code>ai-train</code>, <code>search</code>, <code>ai-input</code> with values of <code>yes</code> or <code>no</code> to steer bots and scrapers, should they respect the value.
** As of 30 September 2025 the IndieWeb wiki implements <code>Content-signal</code> directives.

== Brainstorming ==

=== Types of LLM bot ===

There are several types of bot that act on behalf of or collect data for an LLM. The types of bot vary by service provider, but are mainly distinguished by their intent.

Some bots crawl content (crawlers); others retrieve content for use in answering a user's query ("search bots" or bots that retrieve a web page in service of answering a user's question); others download data for the purpose of training AI models.

Security firm [https://datadome.co/ DataDome] distinguishes 3 types:
* ''<b>Training scrapers</b>, which ingest large volumes of public content to improve model performance. These are typically used to build or refine foundation models and may not honor robots.txt or rate limits.''
* ''<b>Prompt-time fetchers</b>, which retrieve real-time data to supplement LLM outputs—think of AI copilots or search assistants querying web pages on-the-fly to answer user prompts.''
* ''<b>Agentic crawlers</b>, which act more like users. These bots are capable of clicking, scrolling, submitting forms, and navigating complex UIs, often as part of a RAG (retrieval-augmented generation) pipeline or testing framework.''
Source: ''[https://datadome.co/threat-research/ai-agents-llm-crawlers/ AI Agents at the Gate: Understanding & Securing Against LLM Crawlers]''

== See Also ==
* [[large language model]]
* [[robots_txt|robots.txt]]
* [https://darkvisitors.com/agents Dark Visitors keeps a list of robot user agents]. This includes AI bots which are listed in categories like "AI Agent", "AI Assistant", "AI Data Scrapers" and more general bots like "Archivers" and link preview "Fetchers".
* [[common crawl]] is intended to share site data with crawlers so they would avoid inflicting onerous web traffic
* [[large language model]]
* [https://llmstxt.org/ llmstxt.org] is a proposal for a side file called "/llms.txt" ''to provide information to help LLMs use a website at inference time''" but also has secondary recommendations, such as suggesting authors provide a markdown version of all pages on a site, seems to constitute the [[sidefile-antipattern]], and despite having "txt" in the name, is supposed to be written in [[Markdown]]. ''((There may be utility in this, but it seems all over the place in its goals -- {{artlung}}))''
** 2026-01-19 [https://cassidoo.co/post/ai-llm-discoverability/ A fun trick for getting discovered by LLMs and AI tools] by [https://cassidoo.co/ Cassidy Williams]
