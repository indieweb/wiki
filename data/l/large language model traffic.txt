https://indieweb.org/large_language_model_traffic

{{stub}}

'''<dfn>[[large language model|Large language models]] traffic</dfn>''' refers to web requests made by LLM operators, which may be used to crawl data for use in training AI models, or to answer a user's query in a "search" mode (i.e. ChatGPT Search).

Increasing usage of LLM tools corresponds with increased [[crawler]] traffic from web [[robot|robots]]. [[website]] owners have noticed spikes in traffic which have made some public sites unavailable. Some crawlers appear not to respect [[robots txt|robots.txt]] directives.

== IndieWeb Examples ==

Several members of the IndieWeb community have reported AI traffic becoming an increasing burden on their servers.

* {{artlung}} was told by system administrators from his [[hosting|web host]] that bot spikes accounted for multiple crashes of Apache.
* As a result of heavy traffic on the indieweb.org wiki, web server nginx rules were added to prevent access to "diff" pages for individual wiki pages for not-logged in user agents

* ''...add yourself''

== News reporting ==
* 2025-09-08 [https://www.techdirt.com/2025/09/08/were-walling-off-the-open-internet-to-stop-ai-and-it-may-end-up-breaking-everything-else/ We’re Walling Off The Open Internet To Stop AI—And It May End Up Breaking Everything Else]
* 2025-07-07 [https://www.404media.co/the-open-source-software-saving-the-internet-from-ai-bot-scrapers/ The Open-Source Software Saving the Internet From AI Bot Scrapers]
*  2025-03-25 [https://arstechnica.com/ai/2025/03/devs-say-ai-crawlers-dominate-traffic-forcing-blocks-on-entire-countries/ Open source devs say AI crawlers dominate traffic, forcing blocks on entire countries]
* 2025-06-23 [https://scholarlykitchen.sspnet.org/2025/06/23/are-ai-bots-knocking-digital-collections-offline/ Are AI Bots Knocking Digital Collections Offline? An Interview with Michael Weinberg]

== Traffic reports ==
{{section-stub}}
* 2025-09-10 [https://flowingdata.com/2025/09/10/bots-account-for-almost-a-third-of-web-traffic/ Bots account for almost a third of web traffic] (summary of [[Cloudflare]] data)
* 2025-07-21 [https://www.libraryjournal.com/story/ai-bots-swarm-library-cultural-heritage-sites-causing-slowdowns-and-crashes AI Bots Swarm Library, Cultural Heritage Sites, Causing Slowdowns and Crashes]
* [https://library.duke.edu/using/off-campus/impact-of-bots Impact of AI Bots on Library Websites] (Duke University) ''The overwhelming number of simultaneous requests that these AI bots make on our websites has sometimes rendered them inaccessible to our human library patrons.''

== Tools to combat ==
{{section-stub}}

* [https://github.com/TecharoHQ/anubis Anubis] is a "Web AI Firewall Utility". The "[https://anubis.techaro.lol/docs/design/how-anubis-works How Anubis works]" documentation says that Anubis "uses a proof-of-work challenge to ensure that clients are using a modern browser and are able to calculate SHA-256 checksums".
** [https://social.anoxinon.de/@Codeberg/115033796075422170 Codeberg reported that at least one AI crawler was capable of bypassing Anubis],:
<blockquote>…we can confirm that at least Huawei networks now send the challenge responses and they actually do seem to take a few seconds to actually compute the answers… we assume that AI crawlers leveled up their computing power to emulate more of real browser behaviour </blockquote>
* [[Cloudflare]] has a feature called [https://developers.cloudflare.com/bots/additional-configurations/block-ai-bots/ Block AI Bots] which you can use to block AI bots.
* [https://github.com/fail2ban/fail2ban fail2ban] ''bans hosts that cause multiple authentication errors''

== Proposed Tools ==
* [https://blog.cloudflare.com/introducing-pay-per-crawl/ Introducing pay per crawl: Enabling content owners to charge AI crawlers for access]
* [https://rslstandard.org RSL: Really Simple Licensing] ''RSL is an open standard that lets publishers define machine-readable licensing terms for their content, including attribution, pay per crawl, and pay per inference compensation.''

== See Also ==
* [[large language model]]
* [[robots_txt|robots.txt]]
* [https://darkvisitors.com/agents Dark Visitors keeps a list of robot user agents]. This includes AI bots which are listed in categories like "AI Agent", "AI Assistant", "AI Data Scrapers" and more general bots like "Archivers" and link preview "Fetchers".
* [[common crawl]] is intended to share site data with crawlers so they would avoid inflicting onerous web traffic
