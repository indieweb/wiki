https://indieweb.org/robots_txt

{{stub}}
'''<dfn>robots.txt</dfn>''' is a file used to inform web crawlers what parts of a site should or should not be crawled. [https://en.wikipedia.org/wiki/Robots.txt]

Because this file is just a suggestion and bots can choose to ignore it, it's not a guaranteed way of keeping away crawlers. But generally, ones from big search engines will respect it won't publicly index your site if you declare it so.

Example command names:
* [[user-agent|User-agent]]
* [[Disallow]]
* [[Noindex]]
* [[Allow]]
* Crawl-delay
* Sitemap
* Content-signal

== Examples ==
The following examples may be copy pasted into a plain text robots.txt file and placed at the root of your domain.

Brief example to block anything inside a particular top level directory "/wiki/":
<pre>
User-agent: *
Disallow: /wiki/
</pre>

Note that Google seems to ignore the "*" User-agent and must be specifically disallowed:
<pre>
User-agent: Googlebot
Disallow: /wiki/
</pre>

You may want to entirely block some particularly abusive bots:
<pre>
User-agent: AhrefsBot
Disallow: /
</pre>

Directives to disallow [[GPTBot]]: https://platform.openai.com/docs/gptbot/disallowing-gptbot

<pre>
User-agent: GPTBot
Disallow: /
</pre>

Directive to disallow [[ChatGPT]]: https://platform.openai.com/docs/plugins/bot

<pre>
User-agent: ChatGPT-User
Disallow: /
</pre>

Directive to disallow use for Google Bard and Vertex AI generative APIs [https://developers.google.com/search/docs/crawling-indexing/overview-google-crawlers#google-extended]
<pre>
User-agent: Google-Extended
Disallow: /
</pre>

More examples:
* http://pin13.net/robots.txt

== See Also ==
* [[robots]]
* https://www.robotstxt.org/robotstxt.html
* LOL: https://web.archive.org/web/20140702214604/https://www.google.com/killer-robots.txt
* Google crawlerâ€™s implementation of robots.txt: https://developers.google.com/search/docs/advanced/robots/robots_txt
* Google's C++ robots.txt parser: https://github.com/google/robotstxt
* for [[fun]]: https://www.last.fm/robots.txt with details at https://www.wired.com/2010/08/robot-laws/
* [https://coryd.dev/posts/2024/go-ahead-and-block-ai-web-crawlers/ Go ahead and block AI web crawlers]
* [https://darkvisitors.com/agents List of LLM bot user agents]
* [https://github.com/ai-robots-txt/ai.robots.txt/ Github repo maintaining a robots.txt file to block AI agents]
* [[large language model traffic]]
* {{citation
| title = Fix Your robots.txt or Your Site Disappears from Google
| url = https://www.alanwsmith.com/en/37/wa/jz/s1/
| author = [https://www.alanwsmith.com/ Alan W Smith]
| published = 2026-01-07
| archiveurl = https://web.archive.org/web/20260109140622/https://www.alanwsmith.com/en/37/wa/jz/s1/
}} - based on observations by the Shop Talk Show podcast whos traffic has plummeted:
** <blockquote>Your site will be removed from Google search results if you don't have a robots.txt file or the Googlebot site crawler can't access it.</blockquote>
* [https://ignoreallpreviousinstructions.pages.dev/ Ignore all previous instructions] is a single purpose webpage that curates a robots.txt file against AI scrapers
