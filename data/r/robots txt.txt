https://indieweb.org/robots_txt

{{stub}}
'''<dfn>robots.txt</dfn>''' is a file used to inform web crawlers what parts of a site should or should not be crawled.

Because this file is just a suggestion and bots can choose to ignore it, it's not a guaranteed way of keeping away crawlers. But generally, ones from big search engines will respect it won't publicly index your site if you declare it so.

Example command names:
* [[user-agent|User-agent]]
* [[Disallow]]
* [[Noindex]]
* [[Allow]]

== Examples ==
The following examples may be copy pasted into a plain text robots.txt file and placed at the root of your domain.

Brief example to block anything inside a particular top level directory "/wiki/":
<pre>
User-agent: *
Disallow: /wiki/
</pre>

Note that Google seems to ignore the "*" User-agent and must be specifically disallowed:
<pre>
User-agent: Googlebot
Disallow: /wiki/
</pre>

You may want to entirely block some particularly abusive bots:
<pre>
User-agent: AhrefsBot
Disallow: /
</pre>

Directives to disallow [[GPTBot]]: https://platform.openai.com/docs/gptbot/disallowing-gptbot

<pre>
User-agent: GPTBot
Disallow: /
</pre>

Directive to disallow [[ChatGPT]]: https://platform.openai.com/docs/plugins/bot

<pre>
User-agent: ChatGPT-User
Disallow: /
</pre>

More examples:
* http://pin13.net/robots.txt

== See Also ==
* [[robots]]
* https://www.robotstxt.org/robotstxt.html
* LOL: https://web.archive.org/web/20140702214604/https://www.google.com/killer-robots.txt
* Google crawlerâ€™s implementation of robots.txt: https://developers.google.com/search/docs/advanced/robots/robots_txt
* Google's C++ robots.txt parser: https://github.com/google/robotstxt
* for [[fun]]: https://www.last.fm/robots.txt with details at https://www.wired.com/2010/08/robot-laws/
